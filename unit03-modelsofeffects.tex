% MERGE UNITS 03 AND 04 INTO A SINGLE UNIT, FISHERIAN RANDOMIZATION INFERENCE
%  For slides only
%\input{slidesonly}

% % For handout
\input{handout}

%For handout + mynotes
%\input{handout+mynotes}

\input{beamer-preamble-bbh-all}
\input{defs-all}

\title[Fisherian inference]{Unit 3: Fisherian randomization inference and models of effects}
% \author, date moved to beamer-preamble-*-all.tex

\begin{document}


  \begin{frame}
    \frametitle{Outline \& Readings}

\tableofcontents[subsectionstyle=show/hide/hide]

 %  \alert{Readings for this Unit:}\\
 %  \begin{itemize}
 % \end{itemize}

\input{announcement-of-the-day}  % announcement-of-the-day.tex not
                                % part of repo

\end{frame}


\begin{frame}[<+->]{Review: permutation tests vs parametric tests}

When comparing 2 or more groups, one can often choose between
permutation tests and parametric tests. E.g., Fisher vs. $\chi^{2}$ tests.
\begin{itemize}
\item both begin by computing a test statistic
\item they differ in terms of what they compare the statistic to in order to get a p-value
\item parametric tests lean on stronger assumptions
\item permutation tests can use a model to ``motivate'' a test statistic, then compare that test statistic to a permutation-based reference distribution in order committing to the model's assumptions.
\item likewise, the permutational approach circumvents sample-size requirements for p-values' validity.
\end{itemize}

Software notes:
\begin{itemize}
\item In unit 2, $z$'s were permuted using \texttt{sample(z)}.
\item \texttt{sample()} is part of base R, but ``\textrm{shuffle}'' would be more intuitive.
\item The ``mosaic'' add-on package provides a \texttt{shuffle()} command.
\end{itemize}

\end{frame}


\begin{frame}<1-4\mynoteonly>[label=testStats4MeanComparison1Fr]{Test statistics for comparing means with Normal \& non-Normal data}

Stepping back momentarily from issues specific to causal inference, consider the one-way model
$$
Y = \mu + Z\tau + \epsilon,\, \mathbf{E}(\epsilon) = 0.
$$

Suppose the goal is to test the hypothesis that $\tau = 0$.
  
\begin{itemize}[<+->]
\item If $\epsilon \sim \mathcal{N}(0, \sigma^{2})$, some $\sigma >0$, regardless of whether $Z=1$ or 0, the difference of means is optimal, in various senses, as an estimator of $\tau$. By extension, using it as a test statistic gives favorable power.
\item True both for parametric tests (t-test) and permutation tests.
\item If $\epsilon$ follows some other distribution, parametric
  $t$-test's $\alpha$ level is only approximate, depending on CLT. (To
  fix, use permutation $t$-test.)
\item For both versions, \textit{power} suffers: the means reacts so
  strongly to outliers unrelated to treatment that  the ACE becomes
  hard to see.
\item To fix, select a test statistic with built-in outlier
  protection.
\end{itemize}
\end{frame}

\itnote{
\item This time, first 4 bullets only.
}

\section{Models of effects}
\begin{frame}{A simple model of effects for the Acorn GOTV experiment}
  
One simple model is that the GOTV campaign increases voter turnout by $p$ percentage points per precinct.

\begin{center}
  \begin{tabular}{r|rr|rr}
  \hline
 & GOTV? & vote03(\%)& $y_c$ & $y_t$ \\
  \hline
1 & 0 & 38 & 38 & $38+p$\\
$\vdots$& & & & \\
13 & 0 & 19 & 19& $19+p$\\
14 & 0 & 34 & 34& $34+p$\\
15 & 1 & 49 & $49-p$& 49\\
16 & 1 & 38 & $38-p$& 38\\
$\vdots$& & & & \\
28 & 1 & 29 & $29-p$& 29\\
   \hline
\end{tabular}

\end{center}

This family of hypotheses, as $p$ varies between $-100$ and $100$, can be called a \textit{model of effects}.
\end{frame}

\begin{frame}{A more nuanced model for the Acorn GOTV experiment} % if
                                % "more nuanced" changes, drop from
                                % next slide too

Recall that a \textit{response schedule} is a complete specification of unit-level responses to the experiment under every possible random assigment.

It's often more natural to hypothesize only about how treated units would have responded to control, not also how controls would have responded to treatment. For example, here are 3 competing models of the effects of the Acorn GOTV campaign:
\begin{itemize}
\item[No effect] says there was no effect (\texttt{RS0})
\item[one per 10] says the GOTV campaign generated 1 vote for every 10 contacts (\texttt{RS1})
\item[one per 5]says the GOTV campaign generated 1 vote for every 5 contacts (\texttt{RS2})
\end{itemize}

\pause
Note that these models don't specify $y_{t}$ for precincts we only got to observe under control, $z=0$ --- they leave ``blanks'' in the potential schedule.  That's OK.

\end{frame}

\begin{frame}<\nottheirhandout>{Incomplete response schedules and statistical testing}
   An incomplete response schedule can still give us enough to do statistical testing. For example, if the schedule specifies $y_c$ for all subjects, then we can compare the difference in means in $y_c$ under the realized assigment and all possible other assigments.

Using this test statistic for a two-sided test gives approximately

\begin{center}
\begin{tabular}{l|rrr} \hline
& \multicolumn{3}{c}{votes/ 10 contacts}\\
  Hypothesis & 0 & 1 & 2 \\ \hline
$p$ (2-sided) & \visible<2->{.15} & \visible<2->{.38} & \visible<2->{.003} \\ \hline
\end{tabular}
\end{center}

(see \texttt{unit03-Rex.pdf}).
\end{frame}

\note{
Now start \texttt{unit03-Rex}
}

\begin{frame}{Discussion: Fisherian randomization analysis with the ``more nuanced
  model''} % if "more nuanced" drops from prev slide, drop it here too

\begin{itemize}[<+->]
\item When we took the Neyman approach to these data, we decided to
  use \texttt{lm}'s with weights.
  \begin{itemize}
  \item Does the same reasoning apply?
  Does it lead us to the same conclusion?
\item What other consideration should inform this choice?
  \end{itemize}
\item You can use this approach to get a $1-\alpha$ c.i. for the
  rate at which the GOTV campaign generates votes.
  \begin{itemize}
  \item Briefly outline a computer program to do this.
  \item You calculate such an interval and present it as part of a
    research paper. A referee criticizes the paper for ``inappropriate
    statistical assumptions,'' saying that it
    makes a no-interference assumption that's not appropriate, because
    mobilizing voters in one precinct may indirectly mobilize their
    friends or family members in other precincts. How might you respond?
  \end{itemize}
\end{itemize}
\end{frame}



\section{Inference w/ general models of effects}

\begin{frame}{Confidence intervals and models of effects}

  \begin{itemize}[<+->]
  \item The 3 models just considered fall under the broader model that the GOTV
generated $v$ votes per contact, some $v \in [-1, 1]$.
\item Assuming that model, the 95\% CI for $v$ is the collection of $v$s corresponding to incomplete response schedules that would not be rejected at level .05 - a confidence interval by inversion of a family of hypothesis tests.
\item If you want an estimate to go with such a confidence interval, the convention is to report a \textit{Hodges-Lehmann} estimate --- essentially, limit of 100*$(1-\alpha)$\% CIs, as $\alpha \uparrow 1$.
\item There's no precise analogue of the ``standard error.''
\item However, for some of the spirit of a ``standard error'', report a sometimes 2/3 CI alongside a 95\% CI, as suggested by Mosteller \& Tukey (1977, \textit{Data analysis and regression}).
\item We just backed out estimates of the ``Complier average treatment effect,'' and associated confidence intervals, w/o violating the intention-to-treat principle! (Rosenbaum, 1997).
  \end{itemize}


\end{frame}


\begin{frame}
\frametitle{Example: Violence and public infrastructure in Medell{\'i}n, Colombi
a}
\begin{columns}
  \begin{column}{.5\linewidth}
    \begin{itemize}
    \item 2 million residents; 16 districts
    \item Pre-intervention: 60\% poverty rate, 20\% unemployment, homicide 185 p
er 100K
    \item High residential segregation
    \item<2-> 2004-2006: infrastructure intervention for certain poor neighborho
ods.
    \end{itemize}
  \end{column}
  \begin{column}{.5\linewidth}
    \only<1>{\igrphx{medellin-conc-pov}}
\only<2-\mynoteonly>{\igrphx{medellin-gondola}}
  \end{column}
\end{columns}

\end{frame}
\Note{(strata will enter the story because the end analysis involved matching)}



\begin{frame}[fragile]{A model of effects for homicide rates in Medellin}

Each Medellin neighborhood provides several years of non-independent
data.  If we're willing to model Metrocable as a natural experiment,
we can borrow longitudinal data methods, without having to adopt their
dependence assumptions.   For instance, using random
 effects: 

 \begin{enumerate}[<+->]
 \item Let
   $y(T)=$ neighborhood homicide rate in year $T=2002, \ldots, 2008$.  We test models of
   effects of form, for rates $r \leq 0$:
$$H_{0}: y_{t}(T) = \left\{ \begin{array}{lr}\exp((T-2004) r/4)
                              y_{c}(T),& T> 2004; \\
y_{c}(T),& T\leq 2004.\end{array} \right.$$ 
 \item We then fit a random effects model of this form:
\begin{semiverbatim}
 lmer(Count \textasciitilde\  year + (year+1|nh) +\ \pause
            \alert<@+| handout:0>{offset}( nhTrt*(yr>2004)*(r/4)*(yr-2004) ),
          family=poisson, data=homd)
\end{semiverbatim}
\item Fitted params include  ``fixed'' intercept and slope,
  $\hat{\beta}_{0(r)}$ and $ \hat{\beta}_{1(r)}$,\pause  plus a
  ``random'' slope and intercept (also specific to $r$) for
  each n-hood.  
\item To test $H_{0}$, I calculated a test statistic comparing $t$ vs $c$
  n-hoods' fitted intercepts. For the reference dist'n, I rerandomized
  $Z$ and recalculated.
 \end{enumerate}

  
\end{frame}

\begin{frame}<\nottheirhandout>{Outcome analysis for the Medellin study}
  
  \begin{center}
    \igrphx[width=\linewidth]{RIoutcomeAnalysisBarchart}
  \end{center}

\end{frame}

\section{Tests for normal, not Normal, data distributions}


\subsection{Unstratified case}
\againframe{testStats4MeanComparison1Fr}

\begin{frame}{M-estimation using Huber's loss/psi function} \framesubtitle{as a remedy for the difference in means' hypersensitivity}
  \begin{columns}
    \begin{Column}
          \begin{center}
         {\usebeamercolor[fg]{titlelike} Huber vs squared-error loss}
    \end{center}
      \igrphx{huber-loss-fct}
    \end{Column}
    \begin{Column}
          \begin{center}a
         {\usebeamercolor[fg]{titlelike} Huber's psi function}
    \end{center}

\igrphx{Hubers-Psi-Graph}
    \end{Column}
  \end{columns}
\pause
In practice, Huber's M-estimation solves a regression problem in which
the residuals have been top- and bottom-coded, $e \mapsto \max(-t,
\min(t,e))$ at a data-dependent threshold $t>0$.
\end{frame}
\Note{

Tech note: specificially, just as OLS produces residuals that sum to 0
and are uncorrelated with each column of $\mathbf{x}$, the Huber
M-estimate gives residuals which sum to 0 and are uncorrelated with
the xes, \textit{after} the top and bottom coding.
}
\begin{frame}{Rank-based tests } \framesubtitle{as a remedy for the difference in means' hypersensitivity}
  
\end{frame}

\begin{frame}{Workflow for analyzing experiments/obs studies}
  \framesubtitle{To summarize from earlier units:}

  \begin{columns}
    \begin{column}{.25\linewidth}
      
    \end{column}
    \begin{column}{.75\linewidth}
        \begin{itemize}
  \item[Comparison Design] Specify clustering, pairing or blocking
  \item[Estimation] Optionally, compute unbiased estimate of ATE
  \item[MOEs] Specify models of effect; separately for each, figure $\mathbf{y}_{\mathbf{z}\equiv 0} $ or a summary of it.
  \item[Tests \& CIs] Select test statistic, perform h-tests (separately for each MOE). Summarize the results with a 95\% CI, maybe also 2/3 CI. 
  \item[HL estimates] Optionally, Hodges-Lehmann estimate
  \end{itemize}

    \end{column}
  \end{columns}
\end{frame}

\section{Covariance adjustment}
\begin{frame}<\nottheirhandout>{What's missing from this picture?}


  \begin{columns}
    \begin{column}{.25\linewidth}
      
    \end{column}
    \begin{column}{.75\linewidth}
        \begin{itemize}
  \item[Comparison Design] 
  \item[Estimation] 
  \item[MOEs] 
  \item[Tests \& CIs] 
  \item[HL estimates] 
  \end{itemize}

    \end{column}
  \end{columns}
\pause

\vfill
Covariance adjustments to enhance precision, of course.
  
\end{frame}

\begin{frame}{Basic varieties of covariance adjustment}
  
  \begin{itemize}
   \item{} {Differences-in-differences}
  
\item {} {Model-assisted covariance adjustment}

  \end{itemize}

\end{frame}

\begin{frame}{Workflow for analyzing experiments/obs studies}
  \framesubtitle{To summarize from earlier units:}

  \begin{columns}
    \begin{column}{.25\linewidth}
      
    \end{column}
    \begin{column}{.75\linewidth}
        \begin{itemize}
  \item[Comparison Design] Specify clustering, pairing or blocking
  \item[Gain scores] Optionally, adjust for lagged measure of the outcome by subtracting it off from the actual outcome.
  \item[Estimation] Optionally, compute unbiased estimate of ATE
  \item[MOEs] Specify models of effect; separately for each, figure $\mathbf{y}_{\mathbf{z}\equiv 0} $ or a summary of it.
  \item[Cov Adj] Optionally, effect model-assisted covariance adjustments
  \item[Tests \& CIs] Select test statistic, perform h-tests (separately for each MOE). Summarize the results with a 95\% CI, maybe also 2/3 CI. 
  \item[HL estimates] Optionally, Hodges-Lehmann estimate
  \end{itemize}

    \end{column}
  \end{columns}
\end{frame}




\end{document}
