% TODOS:
% limit scope of SE material (introduce idea of SE as an estimate of
% the true SD_0)
% Fill out "probability review" to cover some variance stuff also?
% Remove AE stuff


%  For slides only
%\input{slidesonly}

% % For handout
%\input{handout}

%For handout + mynotes
%\input{handout+mynotes}

\input{beamer-preamble-bbh-all}
\input{defs-all}
\input{courseedition}

\title{Unit 2: Estimation in experiments}
% \author moved to beamer-preamble-*-all.tex

\begin{document}


  \begin{frame}
    \frametitle{Outline \& Readings}

\tableofcontents[subsectionstyle=show/hide/hide]

\input{announcement-of-the-day}  % announcement-of-the-day.tex not
                                % part of repo
\end{frame}






\begin{frame}{Overview of Unit 2, estimating causal parameters}
  \begin{itemize}
  \item In coffee/tea tasting examples, easier to think about whether causation occurred than how one might measure it.
  \item Now, examples that lend themselves to talk of ``average causal effects,'' etc.
%  \item ``effect'' will have a different and more specific meaning than elsewhere in statistics; we'll begin by talking about it.
  \item We'll continue to use methods that don't care whether outcomes
    are binary, continuous, etc, and avoid distributional assumptions.
  \item  First example of a  twist of the study design that the
    analysis has to take into account (clustering).   
  \item We'll add an assumption of \textit{non-interference} (aka ``SUTVA'')
  \item In contrast to unit 1, focus on estimation rather than testing.
  \end{itemize}
\end{frame}


\section{Experiment design \& probability notation} 


\begin{frame}{Common design aspects of randomized studies} 
  \begin{itemize}
  \item<1-> Common properties of small, simple experiments:
    \begin{itemize}
    \item balanced 
    \item complete (e.g., assignment by \texttt{sample()} or by card shuffle)
    \item simple (e.g., card shuffle \textit{or} coin toss)
    \end{itemize}
  \item<2-> Common complications:
    \begin{itemize}
    \item<2-> pairs
    \item<2-> blocks
    \item<2-> cluster-level assignment
    \end{itemize}
  \item<3-> We'll handle complications by simplification, reorganizing so that we're looking at 1 or more simpler studies.
  \item<4-> (Same considerations will apply to non-randomized studies.)
  \end{itemize}

\end{frame}
\note{Examples:
  \begin{itemize}
  \item Fisher/coffee tasting
  \item variations of the tasting experiment envisioned by us, by Neyman
  \item Gosnell, \ldots, Arcenaux GOTV
  \end{itemize}
}


\begin{frame}<1>[label=EEreviewFr]{Review: expected value\footnote{See also G\& G, ch 2}}

  \begin{itemize}
  \item Consider arbitrary \textit{random variables} $V$, $W$.
  \item (For simplicity, assume discrete --- as $\mathbf{Z}$ generally is.)
  \item  $\mathrm{E}(V) = \sum_{\mbox{all v's}} v\mathrm{P}(V=v) $.
  \item<2-> For constant $\alpha$, $\EE  \alpha = \alpha$
  \item<2-> For constants $\alpha, \beta$, $\mathrm{E}(\alpha + \beta V) =
    \alpha + \beta\mathrm{E}(V)$.
  \item<2-> Also $\mathrm{E}(\alpha V + \beta W) = \alpha \mathrm{E}(V) + \beta \mathrm{E}(W)$. 
  \item<3-> In particular, 
$$\mathrm{E}\left(\frac{1}{n} \sum_{i=1}^n V_i\right) = \frac{1}{n} \sum_{i=1}^n \mathrm{E} V_i.$$
  \end{itemize}


\end{frame}
\note{  Note to self: Calculus of expectation items saved for later}

\begin{frame}<\nottheirhandout>{Exercise: random assignment methods}
{\footnotesize (based on G\&G ch1 ex 5)}

\begin{enumerate} \addtocounter{enumi}{8}
\item
\input{assignments/gg15i}
\end{enumerate}

\end{frame}

\againframe<2>{EEreviewFr}




\begin{frame}{Notational conventions for random variables \& vectors}
  
  \begin{itemize}
\item Matrices \& vectors: $(a_{1}, a_{2}, \ldots, a_{n})' = \left(
    \begin{array}{c}
      a_{1} \\ \vdots \\ a_{n}
    \end{array}
\right)$ ;  $\left(
      \begin{array}{c}
        b_{1} \\  \vdots \\ b_{n}\\
      \end{array}
\right)' = (b_{1}, \ldots, b_{n})$.
  \item By convention, $X, Y, Z$ denote random variables (RVs); $x, y, z$, realizations of the RVs.
    \begin{itemize}
    \item Rosenbaum (a statistician) observes this convention\ldots
    \item as I will.
    \item G\& G (political scientists) appear not to.
    \end{itemize}
 \item $x_{i}$= subject $i$ measurement; $\mathbf{x} =(x_{1}, \ldots, x_{n})'$ column of measurements
  \item $Z_i$= random variable for subject $i$; $\mathbf{Z}=(Z_1, Z_2,
    \ldots, Z_n)'$ (a ``random vector'').  
  \item So $\mathbf{z}=(z_1, z_2, \ldots, z_n)'$ denotes a particular
    realization of $\mathbf{Z} $.
  \item Vector cross products: $\mathbf{a}'\mathbf{b}  = a_{1}b_{1} + \cdots + a_{n}b_{n}$
  \end{itemize}


\end{frame}

\begin{frame}<\nottheirhandout>{Exercise: random assignment methods}
{\footnotesize (G\&G ch1 ex 5)}

\begin{enumerate} \addtocounter{enumi}{9}
\item 
{\small \input{assignments/gg15ii}
}
\end{enumerate}
\vfill

% \visible<2>{Either way, canonical analyses treat observations as
% \textit{independent} samples, sizes $n_{0}$ and $n_{1}$, from
% ``\textit{superpopulations}'' of size $N_{0}, N_{1} \approx
% \infty$.  We'll see how one such analysis flows from more serious assumptions.}
\end{frame}


\section{Potential outcome schedules}

\begin{frame}{Four study examples} 


\begin{columns}
\begin{Column}
1. An election campaign randomized (complete) at the precinct level, w/ varying
levels of ``compliance.''\\[2ex]
 2. A quasi-experiment: Hospital admissions on Friday the 13ths vs
Friday the 6ths.\\
{\small
\begin{tabular}{ll|rr} \hline
Yr & Month &	6th &	13th \\ \hline
89 & Oct. &	9 &	13 \\
90 & July &	6 &	12 \\
91 & Sep. &11 &	14 \\
91 & Dec. &	11 &	10 \\
92 & Mar. &	3 &	4 \\
92 & Nov. &	5 &	12 \\ \hline
\end{tabular}
}
\end{Column}
\begin{Column}

3. A natural experiment (w/ simple randomization)%
%\footnote{Example from Gerber and Green (2012)  ch. 2.}
:
\\
        \igrphx{ggch2tab2}\\[2ex]

4. Fisher's tea-tasting experiment. 

\end{Column}
\end{columns}

  
\end{frame}

\begin{frame}{Potential outcomes for the village heads study}
\framesubtitle{under     the hypothesis of     strictly no effect}
  
\end{frame}

\begin{frame}{Two potential outcome schedules for the tea-tasting experiment}%{Two potential outcome schedule for the coffee experiment}

  A \textit{potential outcome schedule} %
%\footnote{G.\&G. ch.2; concept due to Freedman (\textit{Statistical Models\ldots}, C.U.P., 2009).}
is a mapping of assignment vectors $\mathbf{z} = (z_1, \ldots, z_n)'$,
all or mostly counter-to-fact, to outcomes.  Alternatively, a listing of
how each study participant would have responded to
any combination of assignments $\mathbf{z}$ that
the experiment could have produced. Two examples:\\
\begin{columns}
  \begin{Column}
%\hspace{1em} 
    \begin{center}
         {\usebeamercolor[fg]{titlelike} Fisher's null} 
    \end{center}
\begin{tabular}{cccc}
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
1 &   0 \\
1 &   0 \\
1 &   0 \\
1 &   0 \\
0 &   1 \\
0 &   1 \\
0 &   1 \\
0 &   1 \\ \hline
    \end{tabular}
&
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
1 &    0 \\
1 &    0 \\
1 &    0 \\
0 &    0 \\
1 &    1 \\
0 &    1 \\
0 &    1 \\
0 &    1 \\ \hline
    \end{tabular}
&
$\cdots$
&
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
0 &    0 \\
0 &    0 \\
0 &    0 \\
0 &    0 \\
1 &    1 \\
1 &    1 \\
1 &    1 \\
1 &    1 \\ \hline
    \end{tabular}
\end{tabular}
\end{Column}
  \begin{Column}
%\hspace{1em}  
    \begin{center}
       {\usebeamercolor[fg]{titlelike} Perfect discrimination} 
    \end{center}
\begin{tabular}{cccc}
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
1 & 1  \\
1 & 1  \\
1 & 1  \\
1 & 1  \\
0 & 0  \\
0 & 0  \\
0 & 0  \\
0 & 0  \\ \hline
    \end{tabular}
&
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
 1& 1  \\
 1& 1  \\
 1& 1  \\
 0& 0  \\
 1& 1  \\
 0& 0  \\
 0& 0  \\
 0& 0  \\ \hline
    \end{tabular}
&
$\cdots$
& 
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
0 & 0  \\
0 & 0  \\
0 & 0  \\
0 & 0  \\
1 & 1  \\
1 & 1  \\
1 & 1  \\
1 & 1  \\ \hline
    \end{tabular}
  \end{tabular}
\end{Column}
\end{columns}
\pause \medskip

Strict null hypotheses are potential outcome schedules, but not conversely. 

\end{frame}
\itnote{
\item Walk them through -- meanings not self-evident!
}

\begin{frame}{Two potential outcome schedules for the tea-tasting experiment}
\framesubtitle{Compact representation}

\begin{columns}
  \begin{Column}
\hspace{1em}    {\usebeamercolor[fg]{titlelike} Fisher's null} \\
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
$z_1$ &  0   \\
$z_2$ &  0   \\
$z_3$ &  0   \\
$z_4$ &  0   \\
$z_5$ &  1   \\
$z_6$ &  1   \\
$z_7$ &  1   \\
$z_8$ &  1   \\ \hline
    \end{tabular}

  \end{Column}
  \begin{Column}
\hspace{1em}   {\usebeamercolor[fg]{titlelike} Perfect discrimination} \\
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
$z_1$ & $z_1$  \\
$z_2$ & $z_2$  \\
$z_3$ & $z_3$  \\
$z_4$ & $z_4$  \\
$z_5$ & $z_5$  \\
$z_6$ & $z_6$  \\
$z_7$ & $z_7$  \\
$z_8$ & $z_8$  \\ \hline
    \end{tabular}

  \end{Column}

\end{columns}



\end{frame}
\begin{frame}<\nottheirhandout>{Aside: Test statistics \& response schedules}
  
\begin{itemize}
\item One \texttt{test statistic} we discussed for the coffee experiment is the number of ``fancy-coffee'' cups among the cups identified as containing fancy coffee $y=1$. This translates to $Z'Y$. \pause


\item The worksheet (\texttt{unit01-Rex}) has $Z'y$.  This leans on the fact that it assumes Fisher's null, under which $y$ is the same, whatever $Z$ may be. \pause

\item When we're entertaining the possibility of perfect discrimination, we'd have to write $Z'Y$ .
\item We also entertained the treatment-group mean of $y$s as a test
  statistic. To emphasize dependence on $Z$, write this as $Z'y/n_{t}$
  ($Z'Y/n_{t}$). Or, if size of treatment group might be a random
  variable, $Z'y/Z'Z$  ($Z'Y/Z'Z$).
\end{itemize}
\end{frame}

\begin{frame}{Potential outcomes}
  
A much more common way of specifying potential outcome schedules is to
specify \textit{potential outcomes}.  For binary $Z$, and w/ perfect
compliance ($\mathbf{d} \equiv \mathbf{z}$), potential outcomes would be specified as

\begin{columns}
\begin{Column}
    \begin{tabular}{cc} \hline
 $\mathbf{y}_0$ & $\mathbf{y}_1$ \\ \hline
$y_{01}$ & $y_{11}$  \\
$y_{02}$ & $y_{12}$  \\
$y_{03}$ & $y_{13}$  \\
$\vdots$ & $\vdots$  \\
$y_{0n}$ & $y_{1n}$  \\ \hline
    \end{tabular}
\pause
\end{Column}

\begin{Column}
Notes:\\

\begin{itemize}[<+->]
\item (Post-It analogy)
\item Assumes each subject $i$'s response depends only on $d_{i}$, not
  other $d$s.
\item not appropriate for coffee experiment
\item not necessarily appropriate for Salk trial
\item \textit{non-interference} has been assumed
\item \ldots often unwittingly! 
\end{itemize}
  
\end{Column}
\end{columns}

\end{frame}


\section{Expectations and ACEs}


\againframe<3>{EEreviewFr}
\begin{frame}{Expected value of ''$\bar{y}_{0}$''}
  
\end{frame}
\itnote{
\item Rewrite w/ $Z$, $y$
\item Assuming complete RCT (so $(1-Z)'(1-Z)=n_{0}$), apply calc of expectations to remove $Z$s
\item $\frac{1}{n}\sum y_{Ci} = \mu_{0}$ as a parameter 
\item $(1-\mathbf{Z})'\mathbf{y}/n_{0} = \bar{y}_{0}$ as a test statistic, and an estimator
\item Same conclusion for general simple RCTs via conditioning on $Z'Z$.
}

\begin{frame}{Expected value of $\bar{y}_{1}$}
\framesubtitle{In the full compliance case, $D \equiv Z$}
  
\end{frame}
\itnote{
\item If $\mathbf{z} \equiv \mathbf{d}$, then $\bar{y}_{1}$ is also
  unbiased for $\mu_{1}$, under both SRS and independent sampling.
}

\begin{frame}{The ACE ; unbiased estimation}
  
\end{frame}
\itnote{
\item For the moment, assume full compliance, so that $D \equiv Z$.
\item G\&G on why unbiasedness is good: p. 34.
\item A nice thing about this is that the difference of means is
  unbiased for the ACE whatever the (non-interfering) potential
  outcome schedule.
\item DISCUSS VARIANCE FORMULA HERE, OR IN SEPARATE SLIDE?
}

\begin{frame}<\nottheirhandout>{Exercises}
  \begin{enumerate}
  \item Re potential outcomes notation {}(G.\&G. ex.1, ch 1):
    \begin{enumerate}
    \item Explain the notation $Y_{i}(0)$ (as appears in G\&G 2012).
    \item Contrast the meaning of ``$Y(0) | Z=1$'' w/ that of ``$Y(0) | Z=0$''.
    \item How does $\EE(Y(1) | Z=1)$ differ in meaning from $\EE(Y(1))$?
    \item Give an example in which you believe that $\EE(Y(1) | Z=1) =
      \EE(Y(1))$.  Justify your opinion. 
    \item Given an example of real or hypothetical study in which (you
      believe)$\EE(Y(1) | Z=1) \neq \EE(Y(1))$.
    \item Do randomized studies always satisfy $\EE(Y(1) | Z=1)
      = \EE(Y(1))$?  Why or why not?
    \item Do randomized studies always satisfy $\EE(Y(1) | D=1)
      = \EE(Y(1))$?  (What's the difference between this q and
      the last one?)
    \end{enumerate}
\item (G.\&G. ex.4, ch 1) Suppose $z_{i} \in \{0,1\}$, all $i$; assume non-interference. Define the ATT as $\mathbf{z}'(\mathbf{y}_{T}-\mathbf{y}_{C})/\mathbf{z}'\mathbf{z}$.  Prove that if treatments are allocated using complete random assignment, then the ATT is equal in expectation to the average treatment effect.
  \end{enumerate}

\end{frame}
\note{Rewrite me in our notation?  (trim down? Skip?)}

\begin{frame}{The Neyman justification for the 2-sample t-statistic} \pause
 \framesubtitle{I.e., estimating treatment effect as %
$\bar{y}_{1} - \bar{y}_0   \pm \sqrt{\mathrm{SE}(\bar{y}_{1})^{2} +
  \mathrm{SE}(\bar{y}_{0}) ^{2}}$ ($\mathrm{SE}(\bar{y}_{j})^{2} = \sigma_{j}^{2}/n_{j} $) }
\pause

\begin{columns}
  \begin{Column}
    {\usebeamercolor[fg]{titlelike} Canonical approach}  
    \begin{itemize}[<+->]
    \item $(Y_{0i}: i=1, \ldots, n_{0})  $, $(Y_{1i}: i=1, \ldots, n_{1})  $ are
i.i.d., mutually indep., with 2 finite moments. 
    \item $\EE(\bar{Y}_{1} - \bar{Y}_{0}) =
      \EE(\bar{Y}_{1}) - \EE(\bar{Y}_{0}) = \mu_{1} - \mu_{0}$
    \item $\mathrm{Var}(\bar{Y}_{1} - \bar{Y}_{0}) =
      \mathrm{Var}(\bar{Y}_{1}) + \mathrm{Var}(\bar{Y}_{0}) =
      \sigma_{1}/n_{1} + \sigma_{0}/n_{0}$
    \item Finally, $\EE(\frac{1}{n_{j}-1} \sum_{1}^{n_{j}}
      (Y_{ji} - \bar{Y}_{j} )^{2}) = \sigma^{2}_{i}$.
    \end{itemize}
 \end{Column}
  \begin{Column}
    {\usebeamercolor[fg]{titlelike} Neyman's approach}  
    \begin{itemize}[<+->]
    \item Single population of size $N$. Potential outcomes, w/ no
  interference.  T and C groups are simple
   disjoint random samples, sizes $n_{0}$, $n_{1}$.
 \item $\EE(n_{1}^{-1}\mathbf{Z}'\mathbf{y}_{t} - n_{0}^{-1}(\mathbf{1}-
   \mathbf{Z})'\mathbf{y}_{c} ) = \cdots = N^{-1}\sum_{1}^{N} y_{ti} -
   N^{-1}\sum_{1}^{N} y_{ci}$
   \item $\mathrm{Var}(n_{1}^{-1}\mathbf{Z}'\mathbf{y}_{t} - n_{0}^{-1}(\mathbf{1}-
   \mathbf{Z})'\mathbf{y}_{c} ) = \cdots \leq \frac{N}{N-1}
   \left(\frac{\sigma_{1}^{2}}{n_{1}}  +
     \frac{\sigma_{0}^{2}}{n_{0}}\right)$     
\item Finally, $\EE(\frac{1}{\# \mathbf{s}} \sum_{i \in \mathbf{s}}
      (y_{i} - \bar{y}_{\mathbf{s}} )^{2}) = \frac{N}{N-1}\sigma^{2}$.

    \end{itemize}
 \end{Column}
\end{columns}
\pause
Either way, unbiasedness limits error of estimation, error of error of estimation. 
\end{frame}

\itnote{
\item (go over this)
\item Discuss unbiasedness, both ways.
\item Put up Finucan
}

\begin{frame}{A Neyman-compatible justification for the paired t-statistic}
 \framesubtitle{i.e., $\widehat{\mathrm{ACE}} = \bar{d} \pm s_{d}/\sqrt{n}$} 

\end{frame}

\itnote{
\item If $d$'s aren't too crazy then $\hat{\mu}$ is consistent, in triangular array sense
\item Estimating equation formulation
\item The form of the ``robust standard error''
\item Easy to add in weights
\item See also Imai et al 2009 Prop 1 ff
\item (ran out of time for tex-ing)
}
\section{Common estimation targets in comparative studies}

% \begin{frame}{Conditional expectation}
  
%   \begin{itemize}
%   \item $\mathrm{P}(A|B) =  $ conditional probability of A given that B
%   \item $\mathrm{E}(V|B) = \sum_v v\mathrm{P} ((V=v|B)$
%   \item if RVs $W$ and $V$ are independent, then $\mathrm{E}(V|W=w) = \mathrm{E}(V)$.
%   \item Random assignment ensures that $Z$ is independent of $Y_0$, $Y_1$.
%   \item So under random assignment, $\mathrm{E}(Y_0|Z=0) = \mathrm{E}(Y_0)$;  $\mathrm{E}(Y_1|Z=1) = \mathrm{E}(Y_1)$.
%   \item Note that $Z$ is not generally independent of $Y=DY_1 + (1-D)Y_0$!
%   \end{itemize}
% \end{frame}
% \note{
% ``ETT''=$\EE\{\mathrm{ACE}(\mathbf{X})| Z=1\} $}

\begin{frame}{FACEs and ACEs (Holland, 1988, \textit{Soc Meth}) }
\texttt{Revise me to remove conditioning on $\mathbf{x}$}

  \begin{enumerate}[<+->]
  \item   Compare $\EE(Y | Z=1, \mathbf{x})$ to $\EE(Y_{t}| \mathbf{x})$.  In
  an experiment, the same thing -- but how about in an obs study?
\item Likewise  $\EE(Y | Z=0, \mathbf{x})$ vs $\EE(Y_{c}| \mathbf{x})$.
\item ``ACE'' = $\EE(Y_t - Y_c| \mathbf{x}) = \EE(Y_{t}| \mathbf{x}) - \EE(Y_{c}| \mathbf{x})$
\item ``FACE'' = $\EE(Y | Z=1, \mathbf{x}) - \EE(Y | Z=0, \mathbf{x})$
\item In the absence of selection bias, the two coincide:
$$ (Y_{t}, Y_{c}) \perp Z | \mathbf{X}$$
Not ordinarily otherwise.
  \end{enumerate}

Jargon:
``ACE'' above sometimes called ``$\mathrm{ACE}(\mathbf{x})$'',
  in which case $\mathrm{ACE} = \EE\mathtt{ACE}(\mathtt{X}) = \EE\{ \EE(Y_{t}|\mathbf{X}) -
  \EE(Y_{c}|\mathbf{X}) \}$.


\end{frame}

\itnote{
\item Refer to sec 2.6 of G\&G 
}


\begin{frame}{$Z$s/$z$s vs $D$s/$d$s}
  
Additional conventions, this time shared by DOS, G\& G and this course:

\begin{tabular}{cc}
  $Y$ & outcome \\
  $X$ & covariate/baseline variable\\
  $Z$ & treatment assignment\\
  $D$ & treatment received \\
\end{tabular}
\pause

(Each may appear in caps or lowercase, indicating an RV or a realization; each may be bolded to indicate a vector.) \pause

In the coffee experiment, $\mathbf{D} \equiv \mathbf{Z}$; also in experiments discussed in G\& G ch.2.  In general they may diverge (non-compliance). 

\end{frame}


\begin{frame}{Per-protocol and intention to treat}

When there is \textit{non-compliance}, $Z$ and $D$ may differ.  \pause  

  \begin{columns}
    \begin{Column}
  {\usebeamercolor[fg]{titlelike} The per-protocol estimator} \\      
$\frac{1}{\# \{i: D_i = 1\}} \sum_{i:D_i=1} Y_i - \frac{1}{\# \{j: D_j = 0\}} \sum_{j:D_j=0} Y_j$
\bigskip

\uncover<3->{Estimates a FACE-like quantity}
\vspace{.5\textheight} 
\mbox{ }

    \end{Column}
    \begin{Column}
  {\usebeamercolor[fg]{titlelike} The intention-to-treat estimator} \\      
$\frac{1}{\# \{i: Z_i = 1\}} \sum_{i:Z_i=1} Y_i - \frac{1}{\# \{j: Z_j = 0\}} \sum_{j:Z_j=0} Y_j$
\bigskip

\uncover<2->{Estimates the intention to treat effect (the ACE of
  \textit{assignnment} to treatment)}

\vspace{.5\textheight} 
\mbox{ }
    \end{Column}

  \end{columns}
\end{frame}

\begin{frame}{The complier average causal effect via Bloom's(1984) method}
  \begin{itemize}
  \item intention-to-treat effect (Little \& Yau's [1998] $\delta$) vs
    complier average causal effect ($\delta_{c}$).
  \item $\delta = \pi_{c} \delta_{c} + \pi_{n} \delta_{n} $
  \item Assumptions: SUTVA, exclusion, monotonicity.
 \end{itemize}

\igrphx{SalkVtable-full}
\end{frame}

\end{document}


\section{Differences-in-differences}

\begin{frame}{D-in-D for studies with a lagged measure of the outcome}

The \texttt{acorn} data set has

  

\end{frame}

