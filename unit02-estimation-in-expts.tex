%  For slides only
%\input{slidesonly}

% % For handout
%\input{handout}

%For handout + mynotes
%\input{handout+mynotes}

\input{beamer-preamble-bbh-all}
\input{defs-all}
\input{courseedition}

\title{Unit 2: Estimation in experiments}
% \author moved to beamer-preamble-*-all.tex

\begin{document}


  \begin{frame}
    \frametitle{Outline \& Readings}

\tableofcontents[subsectionstyle=show/hide/hide]

\input{announcement-of-the-day}  % announcement-of-the-day.tex not
                                % part of repo
\end{frame}






\begin{frame}{Overview of Unit 2, estimating causal parameters}
  \begin{itemize}
  \item In coffee/tea tasting examples, easier to think about whether causation occurred than how one might measure it.
  \item Now, examples that lend themselves to talk of ``average causal effects,'' etc.
%  \item ``effect'' will have a different and more specific meaning than elsewhere in statistics; we'll begin by talking about it.
  \item We'll continue to use methods that don't care whether outcomes
    are binary, continuous, etc, and avoid distributional assumptions.
  \item  First example of a  twist of the study design that the
    analysis has to take into account (clustering).   
  \item We'll add an assumption of \textit{non-interference} (aka ``SUTVA'')
  \item In contrast to unit 1, focus on estimation rather than testing.
  \end{itemize}
\end{frame}


\section{Experiment design \& probability notation} 


\begin{frame}{Common design aspects of randomized studies} 
  \begin{itemize}
  \item<1-> Common properties of small, simple experiments:
    \begin{itemize}
    \item balanced 
    \item complete (e.g., assignment by \texttt{sample()} or by card shuffle)
    \item simple (e.g., card shuffle \textit{or} coin toss)
    \end{itemize}
  \item<2-> Common complications:
    \begin{itemize}
    \item<2-> pairs
    \item<2-> blocks
    \item<2-> cluster-level assignment
    \end{itemize}
  \item<3-> We'll handle complications by simplification, reorganizing so that we're looking at 1 or more simpler studies.
  \item<4-> (Same considerations will apply to non-randomized studies.)
  \end{itemize}

\end{frame}
\note{Examples:
  \begin{itemize}
  \item Fisher/coffee tasting
  \item variations of the tasting experiment envisioned by us, by Neyman
  \item Gosnell, \ldots, Arcenaux GOTV
  \end{itemize}
}


\begin{frame}<1>[label=EEreviewFr]{Review: expected value\footnote{See also G\& G, ch 2}}

  \begin{itemize}
  \item Consider arbitrary \textit{random variables} $V$, $W$.
  \item (For simplicity, assume discrete --- as $\mathbf{Z}$ generally is.)
  \item  $\EE(V) = \sum_{\mbox{all v's}} v\mathrm{P}(V=v) $.
  \item<2-> For constant $\alpha$, $\EE  \alpha = \alpha$
  \item<2-> For constants $\alpha, \beta$, $\EE (\alpha + \beta V) =
    \alpha + \beta\EE(V)$.
  \item<2-> Also $\EE(\alpha V + \beta W) = \alpha \EE(V) + \beta \EE(W)$. 
  \item<3-> In particular, 
$$\EE\left(\frac{1}{n} \sum_{i=1}^n V_i\right) = \frac{1}{n}
\sum_{i=1}^n \EE V_i.$$
  \end{itemize}


\end{frame}
\note{  Note to self: Calculus of expectation items saved for later}

\begin{frame}<\nottheirhandout>{Exercise: random assignment methods}
{\footnotesize (based on G\&G ch1 ex 5)}

\begin{enumerate} \addtocounter{enumi}{8}
\item
\input{assignments/gg15i}
\end{enumerate}

\end{frame}

\againframe<2>{EEreviewFr}




\begin{frame}{Notational conventions for random variables \& vectors}
  
  \begin{itemize}
\item Matrices \& vectors: $(a_{1}, a_{2}, \ldots, a_{n})' = \left(
    \begin{array}{c}
      a_{1} \\ \vdots \\ a_{n}
    \end{array}
\right)$ ;  $\left(
      \begin{array}{c}
        b_{1} \\  \vdots \\ b_{n}\\
      \end{array}
\right)' = (b_{1}, \ldots, b_{n})$.
  \item By convention, $X, Y, Z$ denote random variables (RVs); $x, y, z$, realizations of the RVs.
    \begin{itemize}
    \item Rosenbaum (a statistician) observes this convention\ldots
    \item as I will.
    \item G\& G (political scientists) appear not to.
    \end{itemize}
 \item $x_{i}$= subject $i$ measurement; $\mathbf{x} =(x_{1}, \ldots, x_{n})'$ column of measurements
  \item $Z_i$= random variable for subject $i$; $\mathbf{Z}=(Z_1, Z_2,
    \ldots, Z_n)'$ (a ``random vector'').  
  \item So $\mathbf{z}=(z_1, z_2, \ldots, z_n)'$ denotes a particular
    realization of $\mathbf{Z} $.
  \item Vector cross products: $\mathbf{a}'\mathbf{b}  = a_{1}b_{1} + \cdots + a_{n}b_{n}$
  \end{itemize}


\end{frame}

\begin{frame}<\nottheirhandout>{Exercise: random assignment methods}
{\footnotesize (G\&G ch1 ex 5)}

\begin{enumerate} \addtocounter{enumi}{9}
\item 
{\small \input{assignments/gg15ii}
}
\end{enumerate}
\vfill

% \visible<2>{Either way, canonical analyses treat observations as
% \textit{independent} samples, sizes $n_{0}$ and $n_{1}$, from
% ``\textit{superpopulations}'' of size $N_{0}, N_{1} \approx
% \infty$.  We'll see how one such analysis flows from more serious assumptions.}
\end{frame}


\section{Potential outcome schedules}

\begin{frame}{Four study examples} 


\begin{columns}
\begin{Column}
1. An election campaign randomized (complete) at the precinct level, w/ varying
levels of ``compliance.''\\[2ex]
 2. A quasi-experiment: Hospital admissions on Friday the 13ths vs
Friday the 6ths.\\
{\small
\begin{tabular}{ll|rr} \hline
Yr & Month &	6th &	13th \\ \hline
89 & Oct. &	9 &	13 \\
90 & July &	6 &	12 \\
91 & Sep. &11 &	14 \\
91 & Dec. &	11 &	10 \\
92 & Mar. &	3 &	4 \\
92 & Nov. &	5 &	12 \\ \hline
\end{tabular}
}
\end{Column}
\begin{Column}

3. A natural experiment (w/ simple randomization)%
%\footnote{Example from Gerber and Green (2012)  ch. 2.}
:
\\
        \igrphx{ggch2tab2}\\[2ex]

4. Fisher's tea-tasting experiment. 

\end{Column}
\end{columns}

  
\end{frame}

\itnote{
\item Muddy point (2016): pairs vs clusters
}

\begin{frame}{Potential outcomes for the village heads study}
\framesubtitle{under     the hypothesis of     strictly no effect}

\begin{tabular}{l|rrr|rrr} \hline
& \multicolumn{6}{c}{Budget share (\%)} \\
& \multicolumn{3}{c}{Observed} & \multicolumn{3}{c}{No effect} \\
Village &$Y_{C}$& $Y_{T}$& $\tau$  &$Y_{C}$& $Y_{T}$& $\tau$ \\ \hline
1&  ? & 15 & ? & 15 & 15 & 0 \\
2& 15 & ? &   ?  & 15 & 15 & 0 \\ 
3& 20 & ? &   ?  & 20 & 20 & 0 \\
4& 20 & ? &   ?  & 20 & 20 & 0 \\
5& 10 & ? &   ?  & 10 & 10 & 0 \\
6& 15 & ? &   ?  & 15 & 15 & 0 \\
7& ?   & 30&? & 30 &30 & 0 \\ \hline
average & 16 & 22.5 & ? & 17.9 & 17.9 & 0 \\ \hline
\end{tabular}
\end{frame}

\begin{frame}{Two potential outcome schedules for the tea-tasting experiment}%{Two potential outcome schedule for the coffee experiment}

  A \textit{potential outcome schedule} %
%\footnote{G.\&G. ch.2; concept due to Freedman (\textit{Statistical Models\ldots}, C.U.P., 2009).}
is a mapping of assignment vectors $\mathbf{z} = (z_1, \ldots, z_n)'$,
all or mostly counter-to-fact, to outcomes.  Alternatively, a listing of
how each study participant would have responded to
any $\mathbf{z}$ that
the experiment could have produced. Two examples:\\
\begin{columns}
  \begin{Column}
%\hspace{1em} 
    \begin{center}
         {\usebeamercolor[fg]{titlelike} Fisher's null} 
    \end{center}
\begin{tabular}{cccc}
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
1 &   0 \\
1 &   0 \\
1 &   0 \\
1 &   0 \\
0 &   1 \\
0 &   1 \\
0 &   1 \\
0 &   1 \\ \hline
    \end{tabular}
&
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
1 &    0 \\
1 &    0 \\
1 &    0 \\
0 &    0 \\
1 &    1 \\
0 &    1 \\
0 &    1 \\
0 &    1 \\ \hline
    \end{tabular}
&
$\cdots$
&
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
0 &    0 \\
0 &    0 \\
0 &    0 \\
0 &    0 \\
1 &    1 \\
1 &    1 \\
1 &    1 \\
1 &    1 \\ \hline
    \end{tabular}
\end{tabular}
\end{Column}
  \begin{Column}
%\hspace{1em}  
    \begin{center}
       {\usebeamercolor[fg]{titlelike} Perfect discrimination} 
    \end{center}
\begin{tabular}{cccc}
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
1 & 1  \\
1 & 1  \\
1 & 1  \\
1 & 1  \\
0 & 0  \\
0 & 0  \\
0 & 0  \\
0 & 0  \\ \hline
    \end{tabular}
&
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
 1& 1  \\
 1& 1  \\
 1& 1  \\
 0& 0  \\
 1& 1  \\
 0& 0  \\
 0& 0  \\
 0& 0  \\ \hline
    \end{tabular}
&
$\cdots$
& 
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
0 & 0  \\
0 & 0  \\
0 & 0  \\
0 & 0  \\
1 & 1  \\
1 & 1  \\
1 & 1  \\
1 & 1  \\ \hline
    \end{tabular}
  \end{tabular}
\end{Column}
\end{columns}



\end{frame}
\itnote{
\item Walk them through -- meanings not self-evident!
\item Strict null hypotheses are potential outcome schedules, but not conversely. 
}

\begin{frame}{Two potential outcome schedules for the tea-tasting experiment}
\framesubtitle{Compact representation}

\begin{columns}
  \begin{Column}
\hspace{1em}    {\usebeamercolor[fg]{titlelike} Fisher's null} \\
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
$z_1$ &  0   \\
$z_2$ &  0   \\
$z_3$ &  0   \\
$z_4$ &  0   \\
$z_5$ &  1   \\
$z_6$ &  1   \\
$z_7$ &  1   \\
$z_8$ &  1   \\ \hline
    \end{tabular}

  \end{Column}
  \begin{Column}
\hspace{1em}   {\usebeamercolor[fg]{titlelike} Perfect discrimination} \\
    \begin{tabular}{cc} \hline
 $\mathbf{z}$ & $\mathbf{y}$ \\ \hline
$z_1$ & $z_1$  \\
$z_2$ & $z_2$  \\
$z_3$ & $z_3$  \\
$z_4$ & $z_4$  \\
$z_5$ & $z_5$  \\
$z_6$ & $z_6$  \\
$z_7$ & $z_7$  \\
$z_8$ & $z_8$  \\ \hline
    \end{tabular}

  \end{Column}

\end{columns}



\end{frame}
\begin{frame}<\nottheirhandout>{Aside: Test statistics \& response schedules}
  
\begin{itemize}
\item One \textit{test statistic} we discussed for the tea experiment
  is the number of milk-first cups ($z=1$) among the cups identified as
  milk-first ($y=1$). I.e., $\mathbf{Z}'\mathbf{Y}$. \pause


\item The worksheet (\texttt{unit01-Rex}) has $\mathbf{Z}'\mathbf{y}$.  This leans on the fact that it assumes Fisher's null, under which $y$ is the same, whatever $Z$ may be. \pause

\item When we're entertaining the possibility of perfect discrimination, we'd have to write $\mathbf{Z}'\mathbf{Y}$ .
\item We also entertained the treatment-group mean of $y$s as a test
  statistic. To emphasize dependence on $Z$, write this as $\mathbf{Z}'\mathbf{y}/n_{1}$
  ($\mathbf{Z}'\mathbf{Y}/n_{1}$). Or, if size of treatment group might be a random
  variable, $\mathbf{Z}'\mathbf{y}/\mathbf{Z}'\mathbf{Z}$  ($\mathbf{Z}'\mathbf{Y}/\mathbf{Z}'\mathbf{Z}$).
\end{itemize}
\end{frame}

\note{Now start the part of unit01-Rex that looks at the acorn study}
\begin{frame}{Potential outcomes}
  
A much more common way of specifying potential outcome schedules is to
specify \textit{potential outcomes}.  For binary $Z$, and w/ full
compliance, potential outcomes would be specified as

\begin{columns}
\begin{Column}
    \begin{tabular}{cc} \hline
 $\mathbf{y}_0$ & $\mathbf{y}_1$ \\ \hline
$y_{01}$ & $y_{11}$  \\
$y_{02}$ & $y_{12}$  \\
$y_{03}$ & $y_{13}$  \\
$\vdots$ & $\vdots$  \\
$y_{0n}$ & $y_{1n}$  \\ \hline
    \end{tabular}
\pause
\end{Column}

\begin{Column}
Notes:\\

\begin{itemize}
\item<1-> (Post-It analogy)
\item<2-> Assumes each subject $i$'s response depends only on $z_{i}$, not
  other $z$s.
\item<2-> not appropriate for coffee experiment
\item<2-> not necessarily appropriate for Salk trial
\item<2-> \textit{non-interference} has been assumed
\item<3-> \ldots often unwittingly! 
\end{itemize}
  
\end{Column}
\end{columns}

\end{frame}


\section[Estimating the ACE]{Estimating the average causal effect}


\againframe<3>{EEreviewFr}
\begin{frame}{Expected values of ''$\bar{y}_{0}$'' \& ''$\bar{y}_{1}$''}
\framesubtitle{in completely randomized designs}
  
\end{frame}
\itnote{
\item Rewrite w/ $Z$, $y$
\item Assuming complete RCT (so $Z'Z=n_{1}$), apply calc of expectations to remove $Z$s
\item $\frac{1}{n}\sum y_{Ti} = \mu_{1}$ as a parameter 
\item $\mathbf{Z}'\mathbf{y}/n_{1} = \bar{y}_{1}$ as a test statistic, and an estimator
\item  (this is for the full compliance case, $D \equiv Z$) 
}

\begin{frame}{Potential outcomes in paired *-experiments}
  
\begin{tabular}{ll|rrrrr} \hline
 &  & \multicolumn{4}{c}{observed} \\
Yr & Mon &   6th &	13th&$y_{Ti2} -y_{Ci1}$ &$y_{Ti1} -y_{Ci2}$\\ \hline
89 & Oct. &	9 &	13  & +4 & ? \\
90 & July &	6 &	12  & +6 & ? \\
91 & Sep. &   11  &	14   & +3 & ?\\
91 & Dec. &   11 &	10  & $-1$& ? \\
92 & Mar. &	3 &	4    & +1& ?\\
92 & Nov. &	5 &	12  & +7& ? \\ \hline
\end{tabular}
\medskip

Assuming $\EE Z_{ij} \equiv .5 $:\\
\begin{align*}
\EE \big( Z_{i1}(y_{Ti1} -y_{Ci2}) + Z_{i2}(y_{Ti2} - y_{Ci1} )
  \big) &=\hspace{.5\linewidth}  \\
\frac{1}{6} \sum_{i=1}^6 \EE \big( Z_{i1}(y_{Ti1} -y_{Ci2}) + Z_{i2}(y_{Ti2} - y_{Ci1} ) \big)  &= \\  
\end{align*}
\end{frame}

\itnote{
\item $\EE \big( Z_{1}(y_{Ti1} -y_{Ci2}) + Z_{i2}(y_{Ti2} - y_{Ci1} )
  \big)$
\item \ldots
\item $\mu_{T} - \mu_{C}$
}

\begin{frame}{Conditional probability, expected value}
  \begin{itemize}
  \item<1-> Consider any discrete random variables $V$, $W$;  any
    \textit{event} $A$ with $\mathrm{Pr}(A) >0 $. 
  \item<1-> \textit{Def.}: $\mathrm{Pr}(V= v | A) = \mathrm{Pr}(V=v,\, A) /\mathrm{Pr}(A) $. 
  \item<2->  $\EE(V) = \sum_{\substack{\mbox{all v's}\\
        \mathrm{s.t.}\, A}} v\mathrm{P}(V=v | A) $.
  \item<3->  It follows that $\EE  (V | A) = (\mathrm{Pr}(A))^{-1} \EE
    (V \mathbf{1}_{A} )$.  Accordingly, the algebra of $\EE ( \, ) $
    applies also to $\EE(\, |A) $:\\
    \begin{align*}
     \EE(\alpha + \beta V| A) &=
    \alpha + \beta\EE(V| A) \\
     \EE(\alpha V + \beta W | A) &=  \alpha \EE(V|A) + \beta \EE(W|A)
      \\
      \EE\big(\frac{1}{n} \sum_{i=1}^n V_i \big| A \big) &= \frac{1}{n} \sum_{i=1}^n \EE (V_i|A).
    \end{align*}
\item<4-> \textit{Ex.} $\mathbf{Z} = (Z_{1}, \ldots, Z_{10}) $ records
  10 indep tosses of a fair coin.\\ (a) $\EE (Z_{1}) = $
  \underline{\hspace{2em}}. (b) $\EE (Z_{1} | \mathbf{Z}'\mathbf{Z}=4)
  =$ \underline{\hspace{2em}}.

  \end{itemize}
\end{frame}

\begin{frame}[t]{Potential outcomes for the village heads study}
\framesubtitle{(with no specific null hypothesis in view)}
  
\begin{columns}
\column{.5\textwidth}
\begin{tabular}{l|rrr|rrr} \hline
& \multicolumn{6}{c}{Budget share (\%)} \\
& \multicolumn{3}{c}{observed} & \multicolumn{3}{c}{all} \\
v. &$Y_{C}$& $Y_{T}$& $\tau$  &$Y_{C}$& $Y_{T}$& $\tau$ \\ \hline
1&     &15 &     & $y_{C1}$ & $y_{T1}$  & $\tau_{1}$\\
2& 15 &   &      & $y_{C2}$ & $y_{T2}$ & $\tau_{2}$\\ 
3& 20 &   &      & $y_{C3}$ & $y_{T3}$ & $\tau_{3}$\\
4& 20 &   &      & $y_{C4}$ & $y_{T4}$ & $\tau_{4}$\\
5& 10 &   &      & $y_{C5}$ & $y_{T5}$ & $\tau_{5}$\\
6& 15 &   &      & $y_{C6}$ & $y_{T6}$ & $\tau_{6}$\\
7&     & 30&     & $y_{C7}$ &$y_{T7}$ & $\tau_{7}$ \\ \hline
avg & $\bar{y}_{0} $ & $\bar{y}_{1}$ &  & $\mu_{C} $ & $\mu_{T}$ & $\mu_{\tau}$ \\ \hline
\end{tabular}
    
\column{.5\textwidth}
$\EE \big( \frac{\mathbf{Z}'\mathbf{Y}}{\mathbf{Z}'\mathbf{Z}}\big|  \mathbf{Z}'\mathbf{Z} \big) =$\\[.5\textheight] 

\mbox{ }
\end{columns}
\vfill

Likewise $\EE \big(
\frac{(1-\mathbf{Z})'\mathbf{Y}}{ (1-\mathbf{Z})'(1-\mathbf{Z})}\big|
\mathbf{Z}'\mathbf{Z} \big) =$ \hfill 
\hfill \mbox{} \\
\end{frame}

\itnote{
\item $\mathbf{Z}'\mathbf{Y} = \mathbf{Z}'\mathbf{y}_{T}$
\item $\EE  \big(
  \frac{\mathbf{Z}'\mathbf{y}_{T}}{\mathbf{Z}'\mathbf{Z}}\big|
  \mathbf{Z}'\mathbf{Z} \big) = \sum_{i} \EE
  \big(\frac{Z_{i}y_{Ti}}{\mathbf{Z}'\mathbf{Z}} \big|
  \mathbf{Z}'\mathbf{Z} \big)$
\item $ \EE \big(\frac{Z_{i}}{\mathbf{Z}'\mathbf{Z}} \big|
  \mathbf{Z}'\mathbf{Z} \big)$
}

\begin{frame}{The ACE ; unbiased estimation}
  
\end{frame}
\itnote{
\item Relate to examples. villages, 
\item G\&G on why unbiasedness is good: p. 34.
\item A nice thing about this is that the difference of means is
  unbiased for the ACE whatever the (non-interfering) potential
  outcome schedule.
}

\begin{frame}{Variances \& estimated variances  }

  \begin{itemize}
 \item A confusing distinction we won't need: $\sigma_{x}^{2} = \frac{1}{n} \sum_{1}^{n} (x_{i} - \bar
   x)^{2}$ vs  $s_{x}^{2} = \frac{1}{n-1} \sum_{1}^{n} (x_{i} - \bar
   x)^{2}$.  We just use $s_{x}^{2} =$\texttt{var(x)}.
 \item Fact: if \texttt{spl}= $(x_{i}: Z_{i}=1) $ in a completely
   randomized design, then $\EE s_{\mathtt{spl}}^{2} = s_{x}^{2} $. 
  \item Confusing distinction we will need: \texttt{var(y)} vs \texttt{var(simT)}.
  \item More necessary confusion: $\mathrm{Var}_{0}$ vs $\mathrm{Var}$ vs $\widehat{\mathrm{Var}} $\ldots
  \end{itemize}

  \begin{center}
    {\small
\begin{tabular}{l|rrr|rrr|rrr} \hline
& \multicolumn{9}{c}{Budget share (\%)} \\
& \multicolumn{3}{c}{Observed} & \multicolumn{3}{c}{No effect} &\multicolumn{3}{c}{all}\\
Village &$Y_{C}$& $Y_{T}$& $\tau$  &$Y_{C}$& $Y_{T}$& $\tau$  &$Y_{C}$& $Y_{T}$& $\tau$ \\ \hline
1    &  ? & 15 & ?  & 15 & 15  & 0 & $y_{C1}$ & $y_{T1}$  & $\tau_{1}$ \\
2    & 15 & ? &   ?  & 15 & 15 & 0 & $y_{C2}$ & $y_{T2}$ & $\tau_{2}$ \\ 
3    & 20 & ? &   ?  & 20 & 20 & 0 & $y_{C3}$ & $y_{T3}$ & $\tau_{3}$ \\
4    & 20 & ? &   ?  & 20 & 20 & 0 & $y_{C4}$ & $y_{T4}$ & $\tau_{4}$ \\
5    & 10 & ? &   ?  & 10 & 10 & 0 & $y_{C5}$ & $y_{T5}$ & $\tau_{5}$ \\
6    & 15 & ? &   ?  & 15 & 15 & 0 & $y_{C6}$ & $y_{T6}$ & $\tau_{6}$ \\
7    & ?   &30&?  & 30 & 30   & 0 & $y_{C7}$ &$y_{T7}$ & $\tau_{7}$ \\ \hline
avg & 16 & 22.5 & ? & 17.9 & 17.9 & 0 & $\mu_{C} $ & $\mu_{T}$ & $\mu_{\tau}$\\ \hline
\end{tabular}
}
  \end{center}
\end{frame}
\itnote{
\item Start by going over variance problems
}

\begin{frame}{The Neyman justification for the 2-sample t-statistic} \pause
 \framesubtitle{I.e., estimating treatment effect as %
$\bar{y}_{1} - \bar{y}_0   \pm \sqrt{\mathrm{SE}(\bar{y}_{1})^{2} +
  \mathrm{SE}(\bar{y}_{0}) ^{2}}$ ($\mathrm{SE}(\bar{y}_{j})^{2} = \sigma_{j}^{2}/n_{j} $) }
\pause

% \begin{columns}
%   \begin{Column}
%     {\usebeamercolor[fg]{titlelike} Canonical approach}  
%     \begin{itemize}[<+->]
%     \item $(Y_{0i}: i=1, \ldots, n_{0})  $, $(Y_{1i}: i=1, \ldots, n_{1})  $ are
% i.i.d., mutually indep., with 2 finite moments. 
%     \item $\EE(\bar{Y}_{1} - \bar{Y}_{0}) =
%       \EE(\bar{Y}_{1}) - \EE(\bar{Y}_{0}) = \mu_{1} - \mu_{0}$
%     \item $\mathrm{Var}(\bar{Y}_{1} - \bar{Y}_{0}) =
%       \mathrm{Var}(\bar{Y}_{1}) + \mathrm{Var}(\bar{Y}_{0}) =
%       \sigma_{1}/n_{1} + \sigma_{0}/n_{0}$
%     \item Finally, $\EE(\frac{1}{n_{j}-1} \sum_{1}^{n_{j}}
%       (Y_{ji} - \bar{Y}_{j} )^{2}) = \sigma^{2}_{i}$.
%     \end{itemize}
%  \end{Column}
%   \begin{Column}
%     {\usebeamercolor[fg]{titlelike} Neyman's approach}  
%     \begin{itemize}[<+->]
%     \item Single population of size $N$. Potential outcomes, w/ no
%   interference.  T and C groups are simple
%    disjoint random samples, sizes $n_{0}$, $n_{1}$.
%  \item $\EE(n_{1}^{-1}\mathbf{Z}'\mathbf{y}_{t} - n_{0}^{-1}(\mathbf{1}-
%    \mathbf{Z})'\mathbf{y}_{c} ) = \cdots = N^{-1}\sum_{1}^{N} y_{ti} -
%    N^{-1}\sum_{1}^{N} y_{ci}$
%    \item $\mathrm{Var}(n_{1}^{-1}\mathbf{Z}'\mathbf{y}_{t} - n_{0}^{-1}(\mathbf{1}-
%    \mathbf{Z})'\mathbf{y}_{c} ) = \cdots \leq \frac{N}{N-1}
%    \left(\frac{\sigma_{1}^{2}}{n_{1}}  +
%      \frac{\sigma_{0}^{2}}{n_{0}}\right)$     
% \item Finally, $\EE(\frac{1}{\# \mathbf{s}} \sum_{i \in \mathbf{s}}
%       (y_{i} - \bar{y}_{\mathbf{s}} )^{2}) = \frac{N}{N-1}\sigma^{2}$.

%     \end{itemize}
%  \end{Column}
% \end{columns}
% \pause
% Either way, unbiasedness limits error of estimation, error of error of estimation. 
\end{frame}

\itnote{
\item (go over this)
}

\begin{frame}{A Neyman-compatible justification for the paired t-statistic}
 \framesubtitle{i.e., $\widehat{\mathrm{ACE}} = \bar{d} \pm s_{d}/\sqrt{n}$} 

\end{frame}

\itnote{
\item If $d$'s aren't too crazy then $\hat{\mu}$ is consistent, in triangular array sense
\item Estimating equation formulation
\item The form of the ``robust standard error''
\item Easy to add in weights
\item See also Imai et al 2009 Prop 1 ff
\item (ran out of time for tex-ing)
}
\section{Caveats and extensions}



\begin{frame}{Average causal effects for samples of populations}
  
  \begin{itemize}
  \item Many writers assume 1,\ldots, n to be a sample from a
  broader population. Then notation $(Y_{T}, Y_{C})$ is appropriate
  (vs $(y_{T}, y_{C})$). 
  \item Random assignment ensures that $Z$ is independent of $Y_0$, $Y_1$.
  \item So under random assignment, $\mathrm{E}(Y_C|Z=0) =
  \mathrm{E}(Y_C)$;  $\mathrm{E}(Y_T|Z=1) = \mathrm{E}(Y_T)$.
  \item W/ simple, complete or paired rand. assmt, $\EE( \bar{Y}_{1}
  - \bar{Y}_{2}) = $ ACE. 
  \item Note that $Z$ is not generally independent of $Y=ZY_1 +
  (1-Z)Y_0$!
  \item Without random assignment, $\EE( \bar{Y}_{1}
  - \bar{Y}_{2}) = $ ACE may fail, even if the sample is perfectly
  representative of the population.
\item This framing is also invoked for ``samples'' from
  ``populations,'' in order to apply apparati of conventional
  frequentist \& Bayesian inference.
  \end{itemize}
\end{frame}

\begin{frame}{FACEs and ACEs (Holland, 1988, \textit{Soc Meth}) }

Example: a university operates 2 oversubscribed charter schools
serving a nearby high-poverty neighborhood. Admission to both is by
lottery; the 2 schools' lotteries are separate, and some families
choose to enter their children in 
both. Let $Z=$ offered admission to at least one.

  \begin{enumerate}[<+->]
  \item   Compare $\EE(Y | Z=1)$ to $\EE(Y_{T})$.  
\item Likewise  $\EE(Y | Z=0)$ vs $\EE(Y_{c})$.
\item ``ACE'' = $\EE(Y_t - Y_c) = \EE(Y_{t}) - \EE(Y_{c})$
\item ``FACE'' = $\EE(Y | Z=1) - \EE(Y | Z=0)$
\item If $\mathrm{Pr}(Z_{i}=1 | (Y_{T}, Y_{C})=(y_{T}, y_{C})) = \pi$
  irrespective of $(y_{T}, y_{C}) $ --- as in simply randomized
  experiments --- then the two coincide.  
Not ordinarily otherwise.
  \end{enumerate}

\pause

(Holland  had nonrandomized studies in mind.  FACEs differ from ACEs
in most observational studies.)

\end{frame}

\itnote{
\item Refer to sec 2.6 of G\&G 
}

\begin{frame}{$Z$s/$z$s vs $D$s/$d$s}
  
Additional conventions, this time shared by DOS, G\& G and this course:

\begin{tabular}{cc}
  $Y$ & outcome \\
  $X$ & covariate/baseline variable\\
  $Z$ & treatment assignment\\
  $D$ & treatment received \\
\end{tabular}
\pause

(Each may appear in caps or lowercase, indicating an RV or a realization; each may be bolded to indicate a vector.) \pause

In the coffee experiment, $\mathbf{D} \equiv \mathbf{Z}$; also in experiments discussed in G\& G ch.2.  In general they may diverge (non-compliance). 

\end{frame}


\begin{frame}{Per-protocol and intention to treat}

When there is \textit{non-compliance}, $Z$ and $D$ may differ.  \pause  

  \begin{columns}
    \begin{Column}
  {\usebeamercolor[fg]{titlelike} The per-protocol estimator} \\      
$\frac{1}{\# \{i: D_i = 1\}} \sum_{i:D_i=1} Y_i - \frac{1}{\# \{j: D_j = 0\}} \sum_{j:D_j=0} Y_j$
\bigskip

\uncover<3->{Estimates a FACE-like quantity}
\vspace{.5\textheight} 
\mbox{ }

    \end{Column}
    \begin{Column}
  {\usebeamercolor[fg]{titlelike} The intention-to-treat estimator} \\      
$\frac{1}{\# \{i: Z_i = 1\}} \sum_{i:Z_i=1} Y_i - \frac{1}{\# \{j: Z_j = 0\}} \sum_{j:Z_j=0} Y_j$
\bigskip

\uncover<2->{Estimates the intention to treat effect (ITT) --- the ACE of
  \textit{assignment} to treatment}

\vspace{.5\textheight} 
\mbox{ }
    \end{Column}

  \end{columns}
\end{frame}

\begin{frame}{The complier average causal effect via Bloom's(1984) method}
  \begin{itemize}
  \item intention-to-treat effect ($\delta$) vs
    complier average causal effect (CACE; $\delta_{c}$).
 \item Although complier/always-taker/never-taker/defier status isn't
   observed, $\delta = p_{c}\delta_{c} + p_{a} \delta_{a} + p_{n} \delta_{n}
   + p_{d} \delta_{d}$.
  \item Assuming no effects for always-takers or never-takers
    (exclusivity), $\delta = p_{c} \delta_{c} + p_{d} \delta_{d} $.
  \item Assuming no defiers (monotonicity), $\delta = p_{c}
    \delta_{c}$. 
  \item both $\delta$ and $p_{c}$ are ACEs of assignment.
 \end{itemize}

\visible<2->{\igrphx{SalkVtable-full}}
\end{frame}

\begin{frame}<\nottheirhandout>{Exercises}
  \begin{enumerate}
  \item Re potential outcomes notation {}(G.\&G. ex.1, ch 1):
    \begin{enumerate}
    \item Explain the notation $Y_{i}(0)$ (as appears in G\&G 2012).
    \item Contrast the meaning of ``$Y(0) | Z=1$'' w/ that of ``$Y(0) | Z=0$''.
    \item How does $\EE(Y(1) | Z=1)$ differ in meaning from $\EE(Y(1))$?
    \item Give an example in which you believe that $\EE(Y(1) | Z=1) =
      \EE(Y(1))$.  Justify your opinion. 
    \item Given an example of real or hypothetical study in which (you
      believe)$\EE(Y(1) | Z=1) \neq \EE(Y(1))$.
    \item Do randomized studies always satisfy $\EE(Y(1) | Z=1)
      = \EE(Y(1))$?  Why or why not?
    \item Do randomized studies always satisfy $\EE(Y(1) | D=1)
      = \EE(Y(1))$?  (What's the difference between this q and
      the last one?)
    \end{enumerate}
\item (G.\&G. ex.4, ch 1) Suppose $z_{i} \in \{0,1\}$, all $i$; assume non-interference. Define the ATT as $\mathbf{z}'(\mathbf{y}_{T}-\mathbf{y}_{C})/\mathbf{z}'\mathbf{z}$.  Prove that if treatments are allocated using complete random assignment, then the ATT is equal in expectation to the average treatment effect.
  \end{enumerate}

\end{frame}

\note{Rewrite me in our notation?  (trim down? Skip?)}


\begin{frame}{Cluster-level assignment}

  \begin{enumerate}
  \item Simple solution: remove clustering by aggregation
\item In effect estimation, be aware of distinctions among\ldots
  \begin{enumerate}
    \item \texttt{sum(z*y)/sum(z)} w/ individual data, $y$= voted?
  \item \texttt{sum(z*y)/sum(z)} w/ clusters,  $y$= precinct turnout rate
  \item \texttt{sum(z*y*pop)/sum(z*pop)} w/ clusters, $y$= precinct turnout rate
  \end{enumerate}
and similarly w \texttt{lm(\ldots)} vs \texttt{lm(\ldots,
  weights=pop)}
\item In estimation of standard errors/calculation of p-values:
  \begin{enumerate}
  \item relevant $n$ is the $n$ of clusters, not of elements
  \item Any software routine that uses element data can't be giving
    you the right answer unless it makes you identify clusters
  \end{enumerate}

  \end{enumerate}

\end{frame}


\end{document}


\section{Differences-in-differences}


